{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5dec91-8ea5-41f6-afae-4bf6073b89f7",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b785d-a6b1-47dc-9ff0-9b6908e8aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale the features of a dataset to a specific range, typically between 0 and 1. \n",
    "It ensures that all features have the same scale and prevents any particular feature from dominating the learning process due to its larger magnitude.\n",
    "\n",
    "The formula to perform Min-Max scaling is as follows:\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "data = [[10], [5], [3], [2], [8]]\n",
    "scaled_data = [[1.0], [0.44444444], [0.22222222], [0.11111111], [0.77777778]]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef728a52-2db0-4994-9253-d083372d479e",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e311f7-7c4d-493e-ac9f-3a09173faeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The Unit Vector technique, also known as normalization or L2 normalization, is a feature scaling method that rescales the feature vectors to have a Euclidean norm of 1. \n",
    "It ensures that all feature vectors have the same scale and direction, making them comparable in terms of their magnitudes.\n",
    "\n",
    "The formula to perform Unit Vector scaling is as follows:\n",
    "scaled_vector = vector / ||vector||\n",
    "\n",
    "The main difference between Unit Vector scaling and Min-Max scaling is the range of the scaled values. While Min-Max scaling rescales the values to a specific range (e.g., between 0 and 1),\n",
    "Unit Vector scaling focuses on normalizing the vectors' directions while keeping the relative magnitudes intact. Unit Vector scaling is commonly used when the magnitude of the feature values\n",
    "is not as important as their relative orientations or angles.\n",
    "\n",
    "Unit Vector scaling is often applied in text classification, document clustering, and recommendation systems, where the direction of feature vectors is significant in determining similarity or relevance.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1918e1-7d60-497b-9c45-a4679a9023f5",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b97eb9-2bda-43dd-95a2-f4dff9745a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional \n",
    "representation while preserving the most important information. It achieves this by identifying the principal components, which are linear \n",
    "combinations of the original features that capture the maximum variance in the data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d140ae15-b8bc-4fdc-bc09-1b457e353fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.19615242e+00  2.56395025e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 5.19615242e+00  2.56395025e-16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.array([[1, 2, 3],\n",
    "                 [4, 5, 6],\n",
    "                 [7, 8, 9]])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "reduced_data = pca.fit_transform(data)\n",
    "\n",
    "print(reduced_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2ca430-5ad9-4c9a-bfc3-cbd6e8752af8",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eaf1cd-0a38-48fc-ab78-4db61a479bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PCA (Principal Component Analysis) can be used for feature extraction, which involves transforming the original features into a new set of derived features \n",
    "(principal components) that capture the most important information in the data.\n",
    "\n",
    "The relationship between PCA and feature extraction lies in the fact that PCA identifies the directions (principal components) along which the data exhibits \n",
    "the maximum variance. These principal components can be seen as new features that are linear combinations of the original features. By selecting a subset of \n",
    "the principal components, we can effectively extract the most informative features from the dataset.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aadfb85b-ee28-4fbc-a213-e724fdcbc3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2)\n",
      "[[-2.68412563  0.31939725]\n",
      " [-2.71414169 -0.17700123]\n",
      " [-2.88899057 -0.14494943]\n",
      " [-2.74534286 -0.31829898]\n",
      " [-2.72871654  0.32675451]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create an instance of PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data using PCA\n",
    "X_transformed = pca.fit_transform(X)\n",
    "\n",
    "print(X_transformed.shape)\n",
    "print(X_transformed[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c978b76-a2d3-4700-a756-00eb16b0e14b",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a4cb8-625e-450d-b478-9e4c155d6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To preprocess the data for a recommendation system using Min-Max scaling:\n",
    "\n",
    "Identify the range of each feature (price, rating, delivery time).\n",
    "Apply Min-Max scaling to rescale the values of each feature to a range between 0 and 1.\n",
    "Use the formula (X - X_min) / (X_max - X_min) to perform the scaling operation.\n",
    "Implement Min-Max scaling using a library like scikit-learn.\n",
    "The scaled data ensures that all features are on a similar scale, preventing any one feature from dominating the others and enabling fair comparison in the recommendation system.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c2a06-9d0f-4db4-8a03-3aebe9cff3ee",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a31920-94ba-46f3-a0c1-da5cc7655bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To reduce the dimensionality of the stock price dataset using PCA:\n",
    "\n",
    "Normalize the dataset by scaling each feature to have zero mean and unit variance.\n",
    "Compute the covariance matrix of the normalized dataset.\n",
    "Perform PCA by eigendecomposition of the covariance matrix to obtain the principal components.\n",
    "Select a subset of the principal components based on their corresponding eigenvalues, which represent the variance explained by each component.\n",
    "Project the original dataset onto the selected principal components, resulting in a lower-dimensional representation that captures the most significant information while reducing noise and redundancy.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3b6a18-b938-4937-be03-af05d6f88f8c",
   "metadata": {},
   "source": [
    "### Question  7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1cc0fe-1441-4497-bcca-e630c88eb49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "values =  [1, 5, 10, 15, 20]\n",
    "scaled_value = (x - min_value) / (max_value - min_value) * 2 - 1\n",
    "scaled_values = [(-1.0, -0.5, 0.0, 0.5, 1.0)]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7017f71-77b9-4ee5-a2bf-64695f560431",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b711c45-8896-48b5-a72f-07d42bac7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To determine the number of principal components to retain in PCA (Principal Component Analysis), you typically consider the explained variance ratio. \n",
    "The explained variance ratio tells us the proportion of the dataset's variance that is explained by each principal component. We aim to retain enough\n",
    "principal components that capture a significant amount of the variance while discarding components that contribute very little.\n",
    "\n",
    "approach to decide the number of principal components to retain:\n",
    "\n",
    "Compute the covariance matrix or correlation matrix of the dataset, depending on whether the features are on different scales or not.\n",
    "Perform the PCA on the dataset and obtain the eigenvalues and eigenvectors.\n",
    "Calculate the explained variance ratio for each principal component by dividing its eigenvalue by the sum of all eigenvalues.\n",
    "Plot the cumulative sum of the explained variance ratios.\n",
    "Choose the number of principal components based on the level of explained variance desired. Commonly, a threshold of around 80% to 95% explained variance is chosen.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
